{
  "document_path": "C:\\Users\\wenming\\source\\repos\\DeepCode\\deepcode_lab\\papers\\3\\3.md",
  "document_type": "research_paper",
  "segmentation_strategy": "semantic_research_focused",
  "total_segments": 9,
  "total_chars": 91043,
  "segments": [
    {
      "id": "f1eecdcc",
      "title": "Abstract",
      "content": "Abstract\nRecent advances in large language models (LLMs) have given rise to powerful\ncoding agents, making it possible for code assistants to evolve into code engineers.\nHowever, existing methods still face significant challenges in achieving high-fidelity\ndocument-to-codebase synthesis—such as scientific papers to code—primarily due\nto a fundamental conflict between information overload and the context bottlenecks\nof LLMs. In this work, we introduce DeepCode, a fully autonomous framework\nthat fundamentally addresses this challenge through principled information-flow\nmanagement. By treating repository synthesis as a channel optimization problem,\nDeepCode seamlessly orchestrates four information operations to maximize task-\nrelevant signals under finite context budgets: source compression via blueprint\ndistillation, structured indexing using stateful code memory, conditional knowl-\nedge injection via retrieval-augmented generation, and closed-loop error correction.\nExtensive evaluations on the PaperBench benchmark demonstrate that DeepCode\nachieves state-of-the-art performance, decisively outperforming leading commer-\ncial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level\nhuman experts from top institutes on key reproduction metrics. By systematically\ntransforming paper specifications into production-grade implementations compara-\nble to human expert quality, this work establishes new foundations for autonomous\nscientific reproduction that can accelerate research evaluation and discovery.\n① Human Expert (Top ML PhD)\n100\n75\n50\n25\n0\n72.4%75.9%\nHuman Expert\nDeepCode② Commercial Code Agents\n100\n75\n50\n25\n0\n40.0%58.7%58.4%84.8%\nCodex\nClaude Code\nCursor\nDeepCode③ Scientific Code Agent\n100\n75\n50\n25\n0\n51.1%73.5%\nPaper Coder\nDeepCode④ LLM-Based Agents\n100\n75\n50\n25\n0\n5.0%7.7%9.8%16.4%35.4%43.3%73.5%\nGemini 2-flash\nGPT-4o\nDeepSeek R1\no3-mini\nClaude 3.5o1\nDeepCode08/12/2025, 22:43 New_UI.html\nﬁle:///Users/lizongwei/Desktop/new_draw/New_UI.html 1/1\nFigure 1: DeepCode main results.\n1",
      "content_type": "introduction",
      "keywords": [
        "fidelity",
        "given",
        "bottlenecks",
        "challenge",
        "lizongwei",
        "code",
        "large",
        "budgets",
        "figure",
        "existing",
        "synthesis",
        "task",
        "framework",
        "conditional",
        "achieves",
        "commercial",
        "autonomous",
        "llms",
        "deepcode",
        "benchmark",
        "expert",
        "fundamentally",
        "blueprint",
        "level",
        "between"
      ],
      "char_start": 297,
      "char_end": 2325,
      "char_count": 2027,
      "relevance_scores": {
        "concept_analysis": 0.6,
        "algorithm_extraction": 0.6,
        "code_planning": 0.7
      },
      "section_path": "Abstract"
    },
    {
      "id": "baad4742",
      "title": "Introduction",
      "content": "Introduction\nThe rapid evolution of Large Language Models (LLMs) has initiated a profound shift in how software\nis specified, implemented, and maintained [ 1,2]. AI-assisted coding tools such as Cursor and Codex\n∗Equal contribution.\n†Chao Huang is the Corresponding Author.\nPreprint. Under review.arXiv:2512.07921v1  [cs.SE]  8 Dec 2025\n\n## Page 2\n\n1. The Aspiration:\nDocument -to-Repository\nAI Agents Human Experts~42%~72%Paper\nReplication scoreThe Reality:\nA Major Performance Gap2. The Core Obstacle:\nInfo Overload vs. Context Bottleneck\nInformation\nOverloa d\nLLM Context\nBottleneck\n1. Specification\nPreservation2. Global\nConsistenc y3. Underspecified\nDesig n4. Executable\nFaithfulnes sThis leads to four key failures3. The Solution: DeepCode's\nInformation -Flow Management\nResult: Surpasses Human Expert\nCursor Human Experts DeepCode\nFour Information Operations\n1. Source\nCompression2. Structured\nIndexing3. Knowledge\nInjection4. Error\nCorrection\nBlueprint CodeMem CodeRAG Verification\nMaximizing relevant signalsFigure 2: From Challenge to Solution of DeepCode. Left: Current AI agents achieve only a 42%\npaper replication score compared to 72% for human experts, highlighting the limitations of existing\nagents. Middle: The core challenge stems from information overload conflicting with LLM context\nlimits, causing four key failure modes. Right: DeepCode addresses this through four information\noperations (Blueprint, CodeMem, CodeRAG, Verification), surpassing human expert performance.\nhave already transformed everyday development practice by automating routine implementation tasks\nand offering intelligent inline suggestions [ 3,4]. Yet these systems remain fundamentally assistive:\nthey operate at the level of code completion, assuming that a human engineer still performs the higher-\nlevel tasks of understanding specifications, planning system architecture, and validating behavior.\nRecent advances in agentic LLM frameworks point toward a more ambitious paradigm—what we\ntermagentic software engineering—in which LLM-based agents are expected to plan, orchestrate, and\nrefine entire software projects from high-level natural language or document-level specifications [ 5,6].\nIn this emerging regime, programming shifts fromwriting codetowriting specifications, and the\ncentral question becomes:can an artificial coding agent behave as an autonomous engineer that\ntranslates rich, informal specifications into comprehensive, robust systems?\nA natural and stringent testbed for this paradigm ishigh-fidelity, document-grounded program\nsynthesis, where a complex scientific paper serves as the sole specification and the goal is to\nproduce a fully executable implementation that faithfully reflects it. Such papers are detailed\nmultimodal specifications, combining informal exposition with equations, pseudo-code, and scattered\nhyperparameters. In this work, we tackle the highly challenging task of reproducing machine\nlearning papers as complete code repositories. Recent efforts have explored this via LLM-based\nagents. PaperBench evaluates frontier models on 20 ICML papers, finding the strongest model (o1)\nwith IterativeAgent achieves only 42.4% replication score, far below 72.4% for human experts [ 7].\nPaperCoder employs a multi-agent pipeline spanning planning, analysis, and generation, reaching\n51.14% reproduction rate on PaperBench [ 8]. These modest results reveal that current approaches fall\nwell short of reliable, end-to-end replication. We identify four key challenges that underlie this gap:\n(i) Specification Preservation.Papers describe the target system through scattered, multimodal\nconstraints. Preserving a faithful mapping from this fragmented specification to implementation is\ninherently difficult.(ii) Global Consistency under Partial Views.Repositories comprise interde-\npendent modules, but generation proceeds file-by-file under limited context. Maintaining consistency\nacross interfaces, types, and invariants under finite context windows easily leads to broken abstrac-\ntions.(iii) Completion of Underspecified Designs.Papers specify only algorithmic cores, leaving\nimplementation details and experimental frameworks implicit. Inferring these consequential but\nunderspecified choices is non-trivial.(iv) Executable Faithfulness.Faithful reproduction requires\nexecutable systems, not just plausible code. Long-horizon generation often yields repositories with\nsubtle logic bugs, dependency conflicts, and fragile pipelines that prevent end-to-end execution.\nWe argue that fundamentally addressing these challenges requiresprincipled information-flow man-\nagement. We abstract the synthesis process as the transmission of a high-entropy specification—the\nscientific paper—through a sequence of bandwidth-constrained channels, defined by the LLM’s\n2\n\n## Page 3\n\ncontext windows. Naive strategies that simply concatenate raw documents with growing code history\ninduce channel saturation, where redundant tokens mask critical algorithmic constraints, causing\nthe effective Signal-to-Noise Ratio to collapse. Consequently, valid repository generation requires a\nparadigm shift governed bycontextual information maximization: at each generation step, the system\nmust actively maximize the density of task-relevant signals while suppressing irrelevant noise.\nMotivated by this perspective, we introduceDeepCode, an open agentic coding framework that\nfundamentally reimagines repository-level synthesis as a problem ofhierarchical information-flow\nmanagement. Rather than treating synthesis as a monolithic process, DeepCode systematically ad-\ndresses the doc-to-repos challenges by instantiating the proposed paradigm through four orchestrated\ninformation operations: (1)source compression, which distills unstructured multi-modal specifica-\ntions into a precise structural blueprint to maximize signal density; (2)structured indexing, which\nabstracts the evolving repository state into concise memory entries to maintain global consistency\nwithout context saturation; (3)conditional knowledge injection, which leverages retrieval-augmented\ngeneration to bridge implicit specification gaps with standard implementation patterns; and (4)error\ncorrection, which utilizes closed-loop verification to transform execution feedback into corrective\nsignals for rectifying transmission errors. Our contributions are threefold:\n•We characterize the task of high-fidelity document-to-repository synthesis through an information-\ntheoretic lens, identifying the central conflict as aninformation-overload vs. context-bottleneck\nconflict. From this perspective, we propose an information-theoretic design principle: effective\nagentic coding systems must explicitly structure, route, and compress information to maximize\ntask-relevant signal under finite context budgets.\n•We instantiate this principle in DeepCode, a systematic framework that orchestrates four strategic\ninformation operations: blueprint distillation, stateful memory management, conditional knowledge\ninjection, and closed-loop verification. By dynamically optimizing the signal-to-noise ratio within\nthe context window, DeepCode effectively resolves the challenges of long-range specification\npreservation, cross-file consistency, and implicit knowledge gaps in complex generation tasks.\n•Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-\nthe-art performance, decisivelyoutperforming leading commercial agents(e.g. Cursor, Claude\nCode, Codex) and, notably,surpassing human expert performanceon key reproduction metrics.\nFurthermore, our analysis reveals that principled information-flow management yields significantly\nlarger performance gains than merely scaling model size or context length, offering a pivotal\ndirection for the future design of autonomous software engineers.\n2 Preliminary\n2.1 Task Definition\nThe primary objective of this work is to develop a system forhigh-fidelity program synthesis. We\nformalize this as the process of learning a mapping function, Fgen, which transforms a specification\ndocument,D, into a complete and executable code repository,P. The core function is defined as:\nFgen:D→P(1)\nwhere Drepresents the space of specification documents and Prepresents the space of valid code\nrepositories. Such that for a given input document D ∈D , the output is a program repository\nP=F gen(D). We address two primary manifestations of this task:\n•Scientific Paper Reproduction:Given a scientific paper from domains such as machine learning\nor computer sciences as the source document D, the system should generate the full source code P\nrequired to replicate the paper’s key experiments and results.\n•Software System Generation:Given a comprehensive technical design document or a concise\nnatural language requirement for a software application (e.g., specifying UI, backend APIs, and\ndatabase schema) as D, the system should generate the corresponding multi-component software\nrepositoryP, including frontend, backend, and configuration files.\nInput: Source Document D.The source document Dis represented as a sequence of multi-modal\nelements, D= (d 1, d2, . . . , d L), where each element dican be a block of text, a mathematical\n3\n\n## Page 4\n\nequation, a table, a figure, or a snippet of pseudocode. The length Lof this sequence is typically\nlarge, posing significant challenges for models with finite context windows.\nOutput: Code Repository P.The target output Pis not a single file but a structured repository. We\ndefine it as a tuple:\nP= (T,C,M)(2)\nHere,Trepresents the directory structure that organizes the files in C.C={c 1, c2, . . . , c N}is a\nset of Nsource code files. The generation of a coherent set Cwhere files correctly interact (e.g.,\nvia imports and function calls) is a non-trivial problem of ensuring cross-file consistency. Mis\nthe dependency manifest (e.g. requirements.txt ,package.json ,README.md file) specifying all\nexternal libraries required to run the code.\n2.2 Objectives\nAn ideal synthesis function Fgenmust generate a repository P∗that optimizes a composite scoring\nfunction. Under our paradigm ofprincipled information-flow management, this optimization is\nframed as maximizing the effective signal-to-noise ratio across the synthesis channel. The optimal\noutput is defined as:\nP∗= arg max\nP∈PScore(P|D)(3)\nTo overcome the conflict between information overload and finite context bandwidth, the scoring\nfunction decomposes into four distinct objectives, each corresponding to an information operation:\n•Specification Preservation:The repository must faithfully implement the rigid algorithmic\nconstraints hidden within the multimodal source document. The objective is to maximize signal\ndensity by extracting precise blueprints from the unstructured input noise.\n•Global Structural Consistency:The generated modules must maintain strict interface compatibil-\nity and type coherence. The objective is to maintain state consistency without context saturation,\nachieved by indexing the evolving codebase into compact, retrievable summaries.\n•Domain Knowledge Grounding:The system must bridge the gap between abstract academic\ndescriptions and concrete engineering implementations. The objective is to resolve underspecified\ndesigns by conditionally injecting standard libraries and patterns from external knowledge bases.\n•Functional Executability:The final repository must be robust and runnable. The objective is to\nminimize transmission errors (bugs) by treating runtime execution feedback as a corrective signal\nto iteratively refine the generated code.\nOur framework is designed to satisfy these objectives by explicitly routing and compressing informa-\ntion, enabling high-fidelity repository generation under strict context window constraints.\n3 The DeepCode Framework\nWe introduce DeepCode, a multi-stage framework designed to instantiate the principle of principled\ninformation-flow management for repository-level synthesis. To solve the optimization problem,\nDeepCode decomposes the generation process into three orchestrated phases, each serving a distinct\ninformation-processing role to maximize the effective signal-to-noise ratio. The process initiates\nwith(1) Blueprint Generation, where a planning agent acts as a source compression mechanism,\ndistilling the high-entropy source document Dinto a structured, high-signal implementation blueprint\nto extract critical constraints while filtering narrative noise. Guided by this blueprint, the subsequent\n(2) Code Generationphase synthesizes source files while preventing channel saturation through\ntwo integrated mechanisms: a stateful Code Memory (CodeMem) that performs structured indexing\nof the evolving codebase to maintain cross-file consistency, and a CodeRAG system that performs\nconditional knowledge injection to bridge implicit domain gaps with standard implementation patterns.\nFinally, the framework concludes with(3) Automated Verification, a closed-loop error correction\nphase where a validation agent treats runtime execution feedback as corrective signals to identify and\nrectify transmission errors, ensuring the functional correctness of the final output.\n4\n\n## Page 5\n\nPhase 3: Automated Verification andRefinement\nStatic Analysis and Code Quality Refinement\nAnalysis Agent\nStatic Analysis of Code Issues\nModification Agent\nLine-level Modifications inspired by LSP\nSandbox Execution and Functional Correction\nSandbox\n Refinement\nTest data\nExecution traceAnalyzing trace\nLSP-based \nrefinementOptimal Code\nRepository\n…\nSource \nDocumentsHierarchical Content Segmentation\nStructural Parsing\nKeyword -chunk Pairs\n…\nConcept Agent Algorithm Agent\nConceptual Analysis Schema Algorithmic Implementation Schema\nPlanning Agent\nPhase 1: Blueprint GenerationCoding Blueprint\nProject File Hierarchy Component Specification\nVerification Protocol Execution EnvironmentStaged\nDev.\nPlanCode Files\nPhase 2: Code GenerationLLMs\nGenerateIterative Code Generation\nCodeRAG CodeMemAdditional\nResources\nRetrieved\nKnowledge\nTarget\nCode FileMemory\nContext\nMemory\nSummarizationUpdate New memory entry\nNext IterationPick from \nBlueprint\nPaper\nDocs\nPassage \nFigure 3: The overall framework of DeepCode.\n3.1 Phase 1: Blueprint Generation\nThe primary goal of the first phase is to perform source compression: distilling the unstructured,\nlengthy content of a source document (e.g. a scientific paper) into a structured, machine-readable\nimplementation blueprint. This distillation process directly mitigates the challenges of information\noverload by transforming the raw input Dinto a high-density signal format. The process begins with\na crucial preprocessing step: hierarchical content segmentation.\n3.1.1 Hierarchical Content Segmentation\nInstead of feeding the entire document Dinto an LLM, we first parse it into a structured representation\nthat facilitates targeted information access. We introduce ahierarchical content index, which\nleverages the inherent structure of academic papers and technical documents. The process is:\n1.Structural Parsing:The source document Dis parsed to identify its hierarchical structure based\non explicit delimiters like section and subsection titles (e.g. \"3. Methodology\", \"3.1. Model\nArchitecture\"). This divides the document into a set of content chunksS={s 1, s2, . . . , s K}.\n2.Keyword-Chunk Association:Each chunk skis stored as a key-value pair (hk, ck), where the\nheading hkserves as a natural, high-level semantic keyword, and ckis the corresponding raw text\ncontent of that section.\nThis indexed structure effectively transforms the problem from one of long-context comprehension\nto a series of more manageable, on-demand retrievals. An agent no longer needs to process the\nentire document at once. Instead, it can query the index using semantic keywords (e.g. requesting the\ncontent associated with \"Model Architecture\") to fetch only the most relevant context for its current\ntask. This approach drastically reduces the token load for any single operation and allows the model\nto focus its limited context window on the most pertinent information, thereby solving the problem of\ncontext overload and information forgetting. This structured representation serves as the foundational\ninput for the specialized agents that perform the detailed analysis in the subsequent steps.\n3.1.2 Multi-Agent Specification Analysis\nFollowing the hierarchical segmentation, we employ a specialized multi-agent system to conduct\na deep and structured analysis of the document’s content. This approach decomposes the complex\ncomprehension task into two parallel tracks, executed by aConcept Agentand anAlgorithm Agent.\nEach agent is equipped with a specific prompt and interacts with the indexed document to extract\ncomplementary layers of information, ensuring a comprehensive understanding without processing\nthe entire document simultaneously.\n5\n\n## Page 6\n\nConcept Agent: High-Level Structural and Conceptual Mapping.The Concept Agent is tasked\nwith building a holistic, high-level understanding of the document. Its primary objective is to map the\npaper’s entire conceptual structure, identify its core scientific contributions, and outline the necessary\ncomponents for a successful experimental reproduction. Operating on the indexed document, the\nagent is instructed to use a segmented reading strategy, querying the index with semantically broad\nkeywords (e.g. “introduction”, “method”). This allows it to assemble a comprehensive overview by\nstrategically fetching relevant sections. The output of this agent is a structuredConceptual Analysis\nSchema. This schema comprises a detailed paper structure map, a method decomposition map\noutlining the system’s core functional components, an implementation map aligning claims with code\nrequirements, and a reproduction roadmap specifying the criteria for success. Collectively, these\nelements translate the paper’s narrative into a structured project plan.\nAlgorithm Agent: Low-Level Technical Detail Extraction.Complementing the conceptual\noverview, the Algorithm Agent is responsible for the meticulous extraction of every low-level\ntechnical detail required for an exact implementation. It’s designed to perform an exhaustive search\nfor all algorithms, mathematical formulations, model architectures, training procedures, and hy-\nperparameters. Moreover, it can leverage online search capabilities to retrieve relevant algorithm\nimplementations from the web as references. Like the Concept Agent, it leverages the segmented read-\ning strategy but uses a distinct set of highly specific keywords (e.g. “algorithm”, “hyperparameter”) to\nperform targeted queries on the most technically dense sections of the document. The agent’s output\nis a granularAlgorithmic Implementation Schema. This schema captures verbatim pseudocode from\nalgorithm boxes, exact mathematical equations and their variables, detailed layer-by-layer network\narchitectures, and a comprehensive list of all hyperparameters with references to their locations in the\npaper. This schema serves as a precise, unambiguous technical specification, designed to leave no\ndetail to interpretation during the code generation phase.\n3.1.3 Synthesizing the Implementation Blueprint\nThe analytical outputs from the Concept and Algorithm agents are then synthesized by theCode\nPlanning Agentinto a single, holistic implementation blueprint. This agent’s critical function\nis to orchestrate the high-level conceptual framework with the low-level technical specifications,\nperforming a final disambiguation and grounding step. It reconciles the architectural overview with\nthe granular implementation details, ensuring that every abstract component is directly linked to a\nprecise technical specification. Should any inconsistencies arise, the agent is authorized to perform\ntargeted queries on the indexed document to resolve them. The finalImplementation Blueprint Bis\na structured intermediate representation designed to be a self-contained, unambiguous specification\nfor code generation. This blueprint is organized into the following canonical sections:\n•Project File Hierarchy:A prioritized project file structure that dictates the logical organization of\nthe codebase and the implementation order of its modules.\n•Component Specification:A granular specification for every module, class, and function, explic-\nitly mapping each to its corresponding algorithmic pseudocode and mathematical formulation.\n•Verification Protocol:A formal plan for validating the final implementation. It defines the\nexperimental setup, specifies the target metrics from the source document, and establishes the\nsuccess criteria for reproduction.\n•Execution Environment:A complete specification of all software dependencies, library versions,\nand requisite hardware configurations needed to compile and run the code.\n•Staged Development Plan:A phased implementation roadmap that defines the build order of\ncomponents and integrates staged verification checks to ensure modular correctness.\nBy consolidating all distilled information into this canonical blueprint, the Code Planning Agent\nconcludes the specification distillation phase. This artifact serves as the definitive \"source of truth\" for\nthe subsequent code generation phase, effectively resolving the long-context challenge by providing\na dense, structured, and actionable input that obviates any need for the coding agents to interact with\nthe original, lengthy document.\n3.2 Phase 2: Code Generation\nUpon generating the high-signal blueprint, the second phase synthesizes the code repository. This\nphase maximizes the density of relevant context while preventing channel saturation caused by the\n6\n\n## Page 7\n\naccumulation of raw code history. A naive iterative approach, which appends previously generated\ncode to the prompt, leads to a collapse in the signal-to-noise ratio and induces hallucinations. To\novercome this, we propose a dual-mechanism strategy for efficient information routing: (1) a stateful\nCodeMemthat performs structured indexing of the evolving repository to maintain internal structural\ncohesion without context bloat, and (2) aCodeRAGsystem that performs conditional knowledge\ninjection, grounding the implementation in external patterns to bridge implicit knowledge gaps.\n3.2.1 Stateful Generation with CodeMem\nThe core of our generation process is the Code Memory mechanism, a strategy designed to maintain a\ncompressed, structured representation of the repository’s state, thereby ensuring cross-file consistency\nwithout suffering from prohibitive context lengths. Instead of passing the full source code of\npreviously implemented files to the generative agent, we iteratively build and query a structured\nmemory bank,M.\nLet the set of all files to be implemented, as defined by Sec. 2, be C={c 1, c2, . . . , c N}. The\ngeneration process is an iterative loop over t= 1, . . . , N . At each step t, we maintain the set of\nimplemented files, Ct−1, and the set of unimplemented files, Ut−1. The process for generating the\ntarget file for the current step,ˆc t, is as follows:\n1.Context Formulation.The generation context for the current step, Xt, is constructed not from\nraw source code, but from the static implementation blueprint Band a dynamically selected subset\nof the Code Memory, Mt−1. The agent first identifies which previously implemented files are\nrelevant to the current target file ˆct(where ˆctdenotes the blank code file to be generated, and ct\ndenotes the resulting generated code file). It then retrieves only their corresponding summaries\nfrom the memory bank:\nXt= (B,SelectRelevantMemory(M t−1,ˆct))(4)\nwhere SelectRelevantMemory is a function that queries Mt−1to fetch only the essential sum-\nmaries of dependencies.\n2.Code Generation.The coding agent, represented by the LLM function L, synthesizes the source\ncode for the target file based on the curated context:\nct=L(X t)(5)\n3.Memory Update.After generating the code ct, the system clears the generation context. A\nspecialized summarization agent, S, is then invoked. This agent analyzes the newly generated\nsource code ctto extract its structural essence and create a new memory entry, mt. The Code\nMemory is then updated:\nMt=M t−1∪ {m t}(6)\nThe summarization agent Sdistills the code into a structured format that captures all information\nnecessary for inter-module communication. Each memory entry mtis a structured object containing:\n•Core Purpose ( Pt):A concise, natural language summary of the file’s primary responsibility and\nrole within the repository.\n•Public Interface ( It):A formal description of all externally accessible classes, functions, and\nconstants, including their signatures and purposes (e.g., Class(params): methods).\n•Dependency Edges ( Et):A comprehensive map of the file’s position within the project’s depen-\ndency graph. This structured entry specifies bothafferent couplings(internal dependencies),\ndetailing the specific imports from other project modules and external packages, and predictedef-\nferent couplings(external dependencies), identifying which unimplemented modules are expected\nto consume this file’s public interface.\n•Next Implementation Target ( ˆct+1):A decision on the next file to be implemented, based on the\nblueprint, dependency graph and the current state. Note that, to avoid introducing noise into the\nmemory, this information is separated fromm tand provided independently as part ofLinput.\nThis mechanism effectively decouples the context size from the repository size. The context provided\nto the agent at any step tremains compact, containing only the high-level blueprint and the highly\ncompressed summaries of relevant, already-implemented files. This stateful, summary-based ap-\nproach allows our system to maintain global consistency and logical cohesion across a large number\nof files, directly solving the long-context and cross-file consistency challenges.\n7\n\n## Page 8\n\n3.2.2 Knowledge Grounding with CodeRAG\nWhile the Code Memory mechanism ensures internal consistency, it does not address the challenges\nof model hallucination or the omission of implicit domain knowledge. To mitigate these issues, we\nintroduce a retrieval-augmented generation framework,CodeRAG, which grounds the synthesis\nprocess in a pre-indexed corpus of relevant, high-quality code repositories. This process is divided\ninto two stages: an indexing phase and an adaptive retrieval phase during code generation.\nRepository Indexing.The goal of this phase is to analyze a set of relevant source code repositories,\nR={R 1, R2, . . . , R K}, and build a structured, queryable index, J. The process, modeled by\nIindex:R × B → J, consists of the following steps:\n1.Relevance Filtering:For each repository Rk∈ R, we perform an initial LLM-based filtering\nto identify a subset of source files, C′\nk⊂R k, that are most relevant to the target project structure\ndefined in the implementation blueprint B. In this context, Rcan denote either the corresponding\nrepository cited in the references of the target paper or other relevant repositories identified through\nonline search. This focuses computational resources on the most promising assets.\n2.Code Understanding:Each relevant source file c′\ns∈ C′\nkis independently analyzed to create\na structured summary, analogous to the memory entries described previously. This summary\ncaptures the file’s purpose, key concepts, and public interfaces.\n3.Relationship Mapping:The core of the indexing process is to establish explicit links between\nthe analyzed source files and the target files in our blueprint. For each source file summary, an\nagent maps it to one or more target files inB, generating a set of relationship tuples.\nThe final output index Jis a structured knowledge base containing a collection of relationship\ntuples. Each tuple is defined as (c′\ns,ˆct, τ, σ, γ) . Here, c′\nsis a file in the source repository and ˆctis\nthe corresponding target file in the blueprint’s structure. τdenotes the relationship type, indicating\nthe nature of the potential contribution, while σis a confidence score representing the strength of\nthe mapping. γis a set of actionable context, such as helpful code snippets, usage suggestions, and\nimplementation patterns.\nAdaptive Retrieval.During the iterative code generation phase, our framework will optionally query\nthe CodeRAG index Jto augment its context. At each generation step tfor a target file ˆct, the agent\nmakes an adaptive decision on whether to retrieve external knowledge. This decision is modeled by a\nbinary functionδ:\nrt=δ(X t,ˆct)(7)\nwhere flag rt∈ {0,1} andXtis the standard context containing the blueprint and relevant code\nmemory. The decision is based on the complexity of the target file and the level of detail available in\nthe blueprint. If rt= 1, the agent queries the index Jto find the most relevant relationship tuples for\nˆct. The retrieved context γfrom the highest-confidence relationship is used to create an augmented\ncontext,X′\nt:\nX′\nt=Xt∪ {Retrieve(J,ˆc t)}(8)\nThe final code is then generated using this enriched context: ct=L(X′\nt). By dynamically incorpo-\nrating proven implementation patterns from existing repositories, CodeRAG significantly reduces\nthe likelihood of generating erroneous or suboptimal code, thus bridging the knowledge gap for the\ngenerative agent.\n3.3 Phase 3: Automated Verification and Refinement\nThe final phase serves as an error correction mechanism to ensure the functional faithfulness of the\nsynthesized repository P. Recognizing that purely generative processes are prone to transmission\nerrors—manifesting as logic bugs, invalid dependencies, or dead code—this phase establishes a\ncrucial closed-loop feedback system absent in standard models. By treating execution outcomes as\ncorrective signals, the framework systematically identifies and rectifies defects through two sequential\nstages: (1) a static analysis pass to ensure structural integrity and code quality, and (2) a dynamic\nexecution pass within a sandboxed environment to enforce functional correctness.\n3.3.1 Static Analysis and Code Quality Refinement\nThe first stage addresses issues that can be detected without executing the code. This process is\norchestrated by a dedicated Analysis Agent and a Modification Agent.\n8\n\n## Page 9\n\nStatic Analysis.An Analysis Agent, denoted by the function Astatic, inspects the generated repository\nPagainst the implementation blueprint B. It produces a structured static analysis report, Rstatic,\nwhich identifies a set of issues. This process can be formalized as:R static=A static(P,B).\nThe identified issues I={i 1, i2, . . . , i K}fall into two categories: i)Structural Discrepancies:This\nincludes integrity violations such as missing files specified in the blueprint or empty (zero-byte)\nsource files that were not correctly generated. ii)Code Quality Deficiencies:The agent leverages\nan LLM to perform a quality assessment of each source file, assigning a quality score, q(ci), and\nflagging sections with poor style, complexity, or maintainability.\nCode Refinement.The report Rstaticis then passed to a Modification Agent, Amodify . This agent iter-\nates through each issue ik∈Iand applies a targeted fix. To perform precise, line-level modifications\nwithout rewriting entire files, the agent utilizes a programmatic interface inspired by the Language\nServer Protocol (LSP). We model this refinement operation as a function ΦLSPthat takes a file ciand\na modification instruction from the report, producing a corrected file c′\ni. The overall process yields a\nstatically refined repositoryP′as:P′=A modify(P,R static).\n3.3.2 Sandbox Execution and Functional Correction\nAfter static refinement, the repository P′undergoes dynamic testing in a secure, isolated sandbox\nenvironment to ensure it runs as intended.\nEnvironment Verification and Setup.A Sandbox Agent, Asandbox , first validates the environment\nsetup instructions (e.g., in README.md ) against the dependencies specified in the blueprint B. Any\ndiscrepancies are corrected. The agent then automatically provisions the specified environment and\ninstalls all dependencies.\nIterative Execution and Correction.The agent then attempts to execute the main entry points of\nthe repository, using automatically generated test data and test files designed to exercise the core\nalgorithms and functions. The execution process, Esandbox , takes the repository P′\njat iteration j\n(initiallyP′\n0=P′) and produces an execution trace,T j, containing all outputs and error messages.\nTj=E sandbox (P′\nj)(9)\nThis initiates an iterative refinement loop. If the trace Tjcontains errors ( Terror\nj̸=∅), the Sandbox\nAgent analyzes the error messages to identify the likely faulty files and the nature of the bug. It then\ngenerates a modification instruction and invokes the LSP-based refinement function ΦLSPto patch the\ncode, producing the repository for the next iteration, P′\nj+1. This loop continues until the execution is\nsuccessful or a maximum number of iterations is reached.\nP′\nj+1= Φ LSP(P′\nj,Terror\nj)(10)\nThe final verified output of our entire framework is the repository P∗=P′\nJ, where Jis the terminal\niteration of the refinement loop. This multi-stage verification and correction process ensures that the\nsynthesized code is not only structurally sound but also functionally correct and conformant to the\noriginal specification.\n4 Experiments\nIn this section, we evaluate the effectiveness of the proposed DeepCode framework by addressing\nthe following 3 research questions:RQ1:How does DeepCode perform compared to existing agent\nframeworks?RQ2:How does the choice of different LLMs affect the performance of DeepCode?\nRQ3:What is the contribution of each module within the DeepCode architecture?\n4.1 Experiments Settings\nDatasets.To evaluate DeepCode’s capabilities in code comprehension and generation, particularly\nfor automated vulnerability detection, we employPaperBench Code-Dev, an innovative benchmark\ncreated by OpenAI [ 7]. PaperBench Code-Dev assesses AI models’ ability to independently reproduce\nleading ML research from major conferences like ICML 2024, focusing on 20 significant papers.\nModels are required to generate all necessary code from scratch, using only the research papers as\nreferences, without accessing existing codebases from the original authors. These tasks are performed\n9\n\n## Page 10\n\nin a virtual machine environment, with the goal of building a functional codebase, replicating\nexperiments, and creating a reproduce.sh script for execution. Each paper is accompanied by a\ndetailed evaluation rubric approved by the authors, which breaks down the reproduction task into 8,316\nspecific, gradable components, meticulously assessed using a hierarchical weighting scheme and\nSimpleJudge, a sophisticated automated judge powered by OpenAI’s o3-mini model. This benchmark\nis rigorously crafted to challenge AI with tasks requiring advanced natural language understanding,\nalgorithmic reasoning, and the ability to generate reliable code from abstract descriptions, all of\nwhich are crucial skills for automating vulnerability detection effectively.\nBaselines.In order to evaluate the effectiveness of the proposed framework, we include a range of\nbaseline methods for comparison. These baselines fall into four distinct categories:\n(1) LLM Agents.We compare against results reported in [ 7] for several state-of-the-art language\nmodels using two agent scaffolding approaches: (1)BasicAgent, a simple tool-use loop based on\nInspect AI’s basic agent that allows models to terminate early, and (2)IterativeAgent, which forces\nmodels to use their full allocated time and employs prompts designed to encourage incremental,\npiecemeal progress. All agents run in Ubuntu 24.04 Docker containers with access to a single A10\nGPU, the internet, and standard development tools including bash, Python, web browsing, and file\nreading capabilities [ 7]. The baseline models include GPT-4o, o1, o3-mini, DeepSeek-R1, Claude\n3.5 Sonnet, and Gemini 2.0 Flash, with most experiments using a 12-hour time limit (extended to 36\nhours for select o1 runs).\n(2) Scientific Code Agents.PaperCoder[ 8]. PaperCoder (also referred to as Paper2Code) is a multi-\nagent LLM framework that transforms machine learning papers into executable code repositories via\na three-stage pipeline: planning, which constructs implementation roadmaps, system architecture\ndiagrams, and file dependencies; analysis, which extracts file-level implementation details; and\ngeneration, which produces modular code in dependency order.\n(3) Commercial Code Agents.We compare against three state-of-the-art commercial code agents\nthat provide AI-powered development assistance through different interfaces and capabilities:\n•Cursor(Version 1.7.52) is an AI-assisted integrated development environment built as a fork of\nVisual Studio Code with additional AI features. Cursor allows developers to choose between\ncutting-edge LLMs and provides codebase embedding models that give agents deep understanding\nand recall [ 9]. In our experiments, Cursor uses Claude Sonnet 4.5-thinking as the underlying model.\n•Claude Code(Version 2.0.22) is Anthropic’s agentic coding tool that lives in the terminal and\nhelps developers turn ideas into code. Claude Code maintains awareness of the entire project\nstructure, can find up-to-date information from the web, and with MCP can pull from external\ndata sources like Google Drive, Figma, and Slack. It can directly edit files, run commands, create\ncommits, and use MCP to read design docs or update tickets [ 10]. Our evaluation uses Claude\nSonnet 4.5-thinking.\n•Codex(Version codex-cli 0.47.0) is OpenAI’s coding agent that runs locally from the terminal\nand can read, modify, and run code on the user’s machine. Codex is optimized for use with\nGPT-5-Codex for agentic coding, with configurable reasoning levels from medium to high for\ncomplex tasks. In auto approval mode, Codex can read files, make edits, and run commands in the\nworking directory automatically [11]. We configure Codex with GPT-5 Codex-high.\n(4) Human Experts.The human baseline [ 7] consists of 8 ML PhD students and graduates from top\ninstitutions (e.g. Berkeley, Cambridge, Carnegie Mellon) who worked part-time over a four-week\nwindow on a 3-paper subset (all-in-one, fre, stay-on-topic). Participants had similar computational\nresources (A10 GPU) and could use AI coding assistants like ChatGPT and GitHub Copilot. The\nbest-of-3 human attempts (Best@3) represent expert-level performance on this subset.\nExperimental Setup.To evaluate DeepCode’s efficacy in high-fidelity repository synthesis, we adopt\na rigorous framework under realistic constraints. The setup combines a secure execution environment\nand the PaperBench protocol for fair, reproducible, detailed comparisons across baselines.\n(1) Implementation Environment.All experiments are conducted within an Ubuntu 22.04 LTS-\nbased sandboxed environment. This infrastructure is provisioned with a standard Python development\nstack and essential dependencies. DeepCode is configured to operate within this isolated space,\nretaining privileges for file system manipulation, shell command execution, and internet access,\nthereby simulating a standard software research and development workflow.\n10\n\n## Page 11\n\n(2) Task Execution.DeepCode accepts the target paper in both PDF and Markdown formats, along\nwith any supplementary addenda, as primary inputs. To ensure that generated solutions stem from\nalgorithmic reasoning rather than retrieval, a source code blacklist is enforced during execution. This\nprotocol precludes access to the authors’ original repositories and known third-party implementations\nduring web browsing. With input parameters defined and the search space constrained, DeepCode\ninitiates its autonomous workflow for code generation and debugging.\n(3) Grading Methodology.Assessment of the generated code follows the PaperBench Code-Dev\nprotocol, which focuses on structural and functional correctness and does not include post-submission\nreproduction. Grading is carried out by SimpleJudge, an automated system based on OpenAI’s\no3-mini, which performs static analysis of the submitted repository against a set of fine-grained,\nhierarchical criteria co-developed with the authors of the source paper. The judging logic is restricted\nto the “Code Development” leaf nodes of this rubric and examines core aspects of software quality,\nincluding static correctness (syntax validity and compliance with language standards), dependency\nvalidity (completeness and correctness of dependency specifications such as requirements.txt ),\nproject structure (coherent and consistent organization of files and directories), and algorithmic\nfidelity (faithful implementation of the algorithms and interfaces described in the original paper).\nThis procedure is designed to align the evaluation with the central technical contributions of the work.\n(4) Evaluation Metrics and Protocol.Our primary evaluation metric is the Replication Score, which\nquantifies the proficiency of DeepCode in translating theoretical concepts into a functional codebase.\nThe score for a single replication trial is derived from the hierarchical rubric through a bottom-up\naggregation process.(i) Leaf node scoring:SimpleJudge first evaluates each leaf node criterion\non a binary basis, assigning a score of 1 for “pass” (compliance) and 0 for “fail” (non-compliance).\n(ii) Score aggregation:The score for any parent node is then computed as the weighted average of\nthe scores of its immediate children. The weights, predetermined during the rubric design, reflect\nthe relative importance of each sub-task.(iii) Final score derivation:This recursive aggregation\ncontinues up the hierarchy until a single score is obtained for the root node, which serves as the\nReplication Score for that trial.\nTo account for the stochasticity inherent in code generation, we adopt a strict evaluation protocol. For\neach target paper, three independent replication trials are performed, and each resulting repository is\nscored separately by SimpleJudge using the procedure described above. The final Replication Score\nis the average of the three scores, mitigating outliers and providing a more stable and reliable measure\nof the model’s typical performance.\n4.2 Main Results\nThe primary results of our experiments are detailed in Figure 4. We analyze the performance\nof DeepCode against the four established categories of baselines: general-purpose LLM agents,\nspecialized scientific code agents, commercial code agents, and human experts.\n•Comparison against LLM Agents.Figure 4 presents average replication scores across all\nbenchmark papers. Among general-purpose LLM agents, performance varies significantly by model\nand scaffolding. With BasicAgent, Claude-3.5-Sonnet achieves the highest score (35.4 ±0.8), while\nother frontier models range from 5.0 to 19.5. IterativeAgent scaffolding improves some models,\nwith o1 reaching the best LLM agent performance of 43.3 ±1.1. DeepCode achieves 73.5 ±2.8,\n/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni00000028/uni0000005b/uni00000053/uni00000048/uni00000055/uni00000057/uni00000027/uni00000048/uni00000048/uni00000053/uni00000026/uni00000052/uni00000047/uni00000048/uni00000013/uni00000015/uni00000018/uni00000018/uni00000013/uni0000001a/uni00000018/uni00000014/uni00000013/uni00000013\n/uni0000001a/uni00000015/uni00000011/uni00000017/uni00000008/uni0000001a/uni00000019/uni00000011/uni0000001a/uni00000008/uni00000014/uni00000011/uni00000003/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni00000028/uni0000005b/uni00000053/uni00000048/uni00000055/uni00000057/uni00000003/uni0000000b/uni00000037/uni00000052/uni00000053/uni00000003/uni00000030/uni0000002f/uni00000003/uni00000033/uni0000004b/uni00000027/uni0000000c\n/uni00000026/uni00000052/uni00000047/uni00000048/uni0000005b\n/uni00000026/uni0000004f/uni00000044/uni00000058/uni00000047/uni00000048/uni00000003/uni00000026/uni00000052/uni00000047/uni00000048/uni00000026/uni00000058/uni00000055/uni00000056/uni00000052/uni00000055\n/uni00000027/uni00000048/uni00000048/uni00000053/uni00000026/uni00000052/uni00000047/uni00000048/uni00000013/uni00000015/uni00000018/uni00000018/uni00000013/uni0000001a/uni00000018/uni00000014/uni00000013/uni00000013\n/uni00000017/uni00000013/uni00000011/uni00000013/uni00000008/uni00000018/uni0000001b/uni00000011/uni0000001a/uni00000008 /uni00000018/uni0000001b/uni00000011/uni00000017/uni00000008/uni0000001b/uni00000018/uni00000011/uni00000017/uni00000008/uni00000015/uni00000011/uni00000003/uni00000026/uni00000052/uni00000050/uni00000050/uni00000048/uni00000055/uni00000046/uni0000004c/uni00000044/uni0000004f/uni00000003/uni00000026/uni00000052/uni00000047/uni00000048/uni00000003/uni00000024/uni0000004a/uni00000048/uni00000051/uni00000057/uni00000056\n/uni00000033/uni00000044/uni00000053/uni00000048/uni00000055/uni00000003/uni00000026/uni00000052/uni00000047/uni00000048/uni00000055/uni00000027/uni00000048/uni00000048/uni00000053/uni00000026/uni00000052/uni00000047/uni00000048/uni00000013/uni00000015/uni00000018/uni00000018/uni00000013/uni0000001a/uni00000018/uni00000014/uni00000013/uni00000013\n/uni00000018/uni00000014/uni00000011/uni00000014/uni00000008/uni0000001a/uni00000016/uni00000011/uni00000019/uni00000008/uni00000016/uni00000011/uni00000003/uni00000036/uni00000046/uni0000004c/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000003/uni00000026/uni00000052/uni00000047/uni00000048/uni00000003/uni00000024/uni0000004a/uni00000048/uni00000051/uni00000057\n/uni0000002a/uni00000048/uni00000050/uni0000004c/uni00000051/uni0000004c/uni00000010/uni00000015/uni00000011/uni00000013/uni00000010/uni00000049/uni0000004f/uni00000044/uni00000056/uni0000004b/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017/uni00000052\n/uni00000027/uni00000048/uni00000048/uni00000053/uni00000036/uni00000048/uni00000048/uni0000004e/uni00000003/uni00000035/uni00000014/uni00000052/uni00000016/uni00000010/uni00000050/uni0000004c/uni00000051/uni0000004c\n/uni00000026/uni0000004f/uni00000044/uni00000058/uni00000047/uni00000048/uni00000003/uni00000016/uni00000011/uni00000018/uni00000003/uni00000036/uni00000052/uni00000051/uni00000051/uni00000048/uni00000057/uni00000052/uni00000014\n/uni00000027/uni00000048/uni00000048/uni00000053/uni00000026/uni00000052/uni00000047/uni00000048/uni00000013/uni00000015/uni00000018/uni00000018/uni00000013/uni0000001a/uni00000018/uni00000014/uni00000013/uni00000013\n/uni00000018/uni00000011/uni00000013/uni00000008/uni0000001a/uni00000011/uni0000001a/uni00000008/uni0000001c/uni00000011/uni0000001b/uni00000008/uni00000014/uni00000019/uni00000011/uni00000017/uni00000008/uni00000016/uni00000018/uni00000011/uni00000017/uni00000008/uni00000017/uni00000016/uni00000011/uni00000016/uni00000008/uni0000001a/uni00000016/uni00000011/uni00000019/uni00000008/uni00000017/uni00000011/uni00000003/uni0000002f/uni0000002f/uni00000030/uni00000010/uni00000025/uni00000044/uni00000056/uni00000048/uni00000047/uni00000003/uni00000024/uni0000004a/uni00000048/uni00000051/uni00000057/uni00000056\nFigure 4: Comparison of DeepCode with four baseline categories: (1) human experts, (2) state-of-\nthe-art commercial code agents, (3) scientific code agents, and (4) LLM-based agents\n11\n\n## Page 12\n\nrepresenting a 70% relative improvement over the best LLM agent baseline. This substantial gap\ndemonstrates that our framework’s specialized design, which incorporates systematic planning,\nstructured code generation and automated verification, provides significant advantages over general-\npurpose agent scaffolding.\n•Comparison against Scientific Code Agents.PaperCoder, a specialized multi-agent framework\ndesigned for transforming machine learning papers into executable code, achieves a score of\n51.1±1.4, outperforming all LLM agents baselines. However, DeepCode achieves a significantly\nhigher score of 73.5 ±2.8—an improvement of over 22 points. This substantial gain suggests that\nour approach to task decomposition, code generation, and repository-level integration is markedly\nmore effective than existing specialized methods.\n•Comparison against Commercial Code Agents.Table 1 details a direct comparison with\nleading commercial agents on a 5-paper subset. DeepCode achieves an average score of 0.8482,\ndecisively outperforming Codex (0.3997), Cursor (0.5841), and Claude Code (0.5871). This result is\nparticularly noteworthy: DeepCode uses the same base model as both Cursor and Claude Code. The\ndramatic performance difference provides strong evidence that our framework’s performance gains\nare not merely a product of a powerful base model. Rather, the advantage is directly attributable to\nthe superior agentic architecture, planning, and execution strategies of DeepCode.\n•Comparison against Human Experts.The most compelling finding is the comparison to human\nexpert performance. As shown in the final rows of Figure 4, we benchmarked performance on\nthe 3-paper subset. The human baseline, which represents the best-of-3 attempts from ML PhD\nstudents, achieved a score of 72.4. Our DeepCode’s average performance on this same subset was\n75.9±4.5, meaning it not only competes with but exceeds the score of the best attempt from a\nhuman expert. This result strongly validates our approach, demonstrating its capability to automate\nand even surpass expert-level performance on this highly challenging task.\nTable 1: Reproduction scores of DeepCode and commercial code agents on 5-paper subset\nModel fre rice bam pinn mech-u Avg.\nCodex (GPT 5 Codex-high) 0.4095 0.3645 0.1937 0.5382 0.4926 0.3997\nClaude Code (Claude Sonnet 4.5-think) 0.6286 0.3787 0.3829 0.7233 0.8222 0.5871\nCursor (Claude Sonnet 4.5-think) 0.6344 0.4186 0.3779 0.7748 0.7148 0.5841\nDeepCode(Claude Sonnet 4.5-think)0.8435 0.7380 0.8530 0.9474 0.8888 0.8541\n4.3 Analysis on Different LLMs\nWe evaluate DeepCode with five LLM backbones (Claude-4.5-Sonnet, GPT-5, Claude-3.5-Sonnet,\nGemini-2.5-Pro, DeepSeek-R1) on three PaperBench tasks (fre, all-in-one, stay-on-topic). The\ntasks vary in specification complexity: fre and all-in-one contain long, interdependent setups with\noverlapping constraints, while stay-on-topic provides more structured descriptions. Agent architecture\nand tooling remain constant to isolate model capability effects.\nAs shown in Figure 5, reproduction scores exhibit consistent stratification across all three tasks.\nClaude-4.5-Sonnet achieves the best or near-best performance (0.72-0.82), demonstrating particular\nstrength on fre and all-in-one where it more reliably reconstructs implementation details and multi-\nstage pipelines implied by complex, underspecified descriptions. GPT-5 tracks Claude-4.5-Sonnet\nclosely on most metrics (0.69-0.81) and shows marginal advantages on stay-on-topic (0.81 vs.\n0.72), suggesting additional robustness in maintaining alignment with fixed experimental framings,\nthough this does not overturn Claude-4.5-Sonnet’s overall dominance. Mid-tier models occupy an\nintermediate performance range: Claude-3.5-Sonnet (0.48-0.57) and Gemini-2.5-Pro (0.44-0.73)\nsuccessfully recover main experimental skeletons but leave notable gaps in finer-grained procedural\nsteps. DeepSeek-R1 consistently underperforms ( ≈0.29), reproducing only fragments of target\nworkflows across all tasks. This stable ranking pattern across heterogeneous specifications indicates\nthat under fixed agent architecture, the underlying language model becomes the primary factor\ndetermining the ceiling and reliability of automatic paper-level reproduction.\n12\n\n## Page 13\n\n0.0 0.2 0.4 0.6 0.8 1.0DeepSeek-R1Gemini-2.5-proClaude-3.5-sonnetGPT-5Claude-4.5-sonnet\n0.2930.7250.5200.7730.823fre\n0.0 0.2 0.4 0.6 0.8 1.0\nReplication Score0.2870.4400.5700.6940.758all-in-one\n0.0 0.2 0.4 0.6 0.8 1.00.2930.5250.4800.8120.720stay-on-topicDeepSeek-R1 Gemini-2.5-pro Claude-3.5-sonnet GPT-5 Claude-4.5-sonnetFigure 5: DeepCode reproduction results on the 3-paper subset across LLM backbones\n4.4 Ablation Studies\nIn this section, we conduct ablation studies on three core components of DeepCode: CodeRAG,\nCodeMem, and Automated Verification. Specifically, we evaluate CodeRAG and Automated Verifica-\ntion on a 3-paper subset (all-in-one, fre, stay-on-topic), while CodeMem is assessed on 5 randomly\nselected tasks (test-time-model-adaptation, rice, mechanistic-understanding, fre, all-in-one). Our key\nfindings are summarized as follows.\n(1) Impact of CodeRAG.To decouple the impact of CodeRAG, we conducted an ablation study using\nGemini-2.5-Flash. As visualized in Figure 6a, the integration of CodeRAG delivers a substantial\nperformance leap (up to 70% relative gain), effectively breaking the base model’s performance ceiling\n(0.35–0.38). Notably, we observed negligible gains when applying CodeRAG to frontier models\nlike Claude 4.5 Sonnet. This contrast yields a critical insight: while reasoning giants likely encode\nsufficient implementation patterns within their parameters, cost-efficient models like Flash suffer\nfrom inherentknowledge gaps. Consequently, CodeRAG proves indispensable for these architectures,\nacting as a vital bridge to fill implicit domain voids with standard practices—confirming that external\nknowledge injection is essential for democratizing high-fidelity replication on lightweight models.\n(2) Impact of CodeMem.We ablate CodeMem’s contribution on five PaperBench tasks using\nClaude-4.5-Sonnet, comparing DeepCode’s structured memory against a \"Simple\" baseline that\nnaively evicts historical messages via sliding windows when approaching context limits.\nResults demonstrate that unstructured eviction causes context saturation with signal loss: the Simple\nprotocol achieves only 0.33-0.43 in rice, fre, and mechanistic-understanding tasks due to dependency\ntruncation, where foundational class definitions are discarded before dependent code generation.\nCodeMem’s structured indexing maintains task-relevant signal density, restoring scores to 0.70-0.92\nby preserving critical dependencies without exhausting context budgets. Even in scenarios with strong\nbaseline performance (test-time-model-adaptation: 0.62 →0.72; all-in-one: 0.66 →0.76), Structured\nmemory delivers consistent gains, confirming our core thesis: effective agentic coding requires\nexplicit information flow management to maximize signal-to-noise ratio under context constraints.\n(3) Impact of Automated Verification.Across 3 test papers, Automated Verification yields consistent\ngains of 3.7–6.5%, elevating scores from 0.69–0.81 to 0.73–0.84. The layer primarily corrects three\ntypes of residual errors: typos in variable names, missing dependencies, and wrong command-line\narguments. These errors prevent otherwise sound implementations from executing reliably. The\n0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70\nReplication Score (CodeRAG)fre\nall-in-one\nstay-on-topic+68.8%0.380 0.642\n+41.0%0.354 0.499\n+71.3%0.360 0.616- CodeRAG\nCodeRAG\n- Verification\nVerification0.650 0.675 0.700 0.725 0.750 0.775 0.800 0.825 0.850Replication Score (Verification)\n+3.7%0.8136 0.8435\n+5.4%0.7193 0.7585\n+6.5%0.6895 0.7342\n(a) Ablation of CodeRAG and Verification\ntest-time-model\nadaptationrice\nmechanistic\nunderstanding\nfre\nall-in-one0.20.40.60.81.0Simple\nCode Memory (b) Ablation of CodeMem\nFigure 6: Ablation studies of key components in DeepCode on PaperBench\n13\n\n## Page 14\n\nmodest improvement reflects an important fact: the earlier phases have already achieved technical cor-\nrectness. Verification is a final pass to ensure reliable execution. It eliminates small but consequential\ndeviations that cause borderline implementations to fail, transforming them into faithful replications.\n5",
      "content_type": "introduction",
      "keywords": [
        "redundant",
        "scaffolding",
        "workflows",
        "criteria",
        "potential",
        "versions",
        "reliable",
        "subsection",
        "outline",
        "decouples",
        "achieves",
        "hierarchy",
        "similar",
        "computer",
        "benchmark",
        "successful",
        "immediate",
        "internet",
        "built",
        "preprint",
        "resulting",
        "approaching",
        "codemem",
        "judging",
        "replication"
      ],
      "char_start": 2325,
      "char_end": 58061,
      "char_count": 55735,
      "relevance_scores": {
        "concept_analysis": 0.9,
        "algorithm_extraction": 1.0,
        "code_planning": 0.9
      },
      "section_path": "Introduction"
    },
    {
      "id": "51d42e55",
      "title": "Related Work",
      "content": "Related Work\n5.1 General Coding Agents\nThe field of software engineering is being rapidly transformed by agentic systems that have evolved\nfrom passive code assistants into autonomous entities capable of planning, executing multi-step\ntasks, and self-correction [ 4,2]. Research has explored several key architectures for these agents.\nOne prominent trend involves multi-agent frameworks that emulate human development teams.\nThis includes systems like ChatDev [ 12], MetaGPT [ 13], and CodePoRi [ 14], which simulate entire\nsoftware company organizational structures to manage development tasks from scratch. For repo-level\ncode generation, CodeS [ 15] proposed to decompose repository generation into specialized agents\nfor structure planning and content filling. AgentCoder [ 16] employs atest-driven refinement loop\ninvolving programmer, test designer, and test executor agents, while MapCoder [ 17] mirrors human\nprogram synthesis with four agents handling example retrieval, planning, generation, and debugging.\nA second major trend focuses on enhancing agents with specialized tools and interfaces. For instance,\nCodeAgent [ 18] integrates five domain-specific tools to support repository-level analysis, while SWE-\nagent [ 19] introduces a high-level Agent-Computer Interface (ACI) to enable robust agent interaction\nwith file systems and development environments. In addition, ToolGen [ 20] proposes representing\neach tool as a unique token and directly integrating tool-specific knowledge into the parameters of the\nLLM, thereby enabling a paradigm shift toward seamless unification of tool invocation and natural\nlanguage generation.\nRecent advancements in academic research are increasingly being translated into practical, produc-\ntized tools. Commercial code agents emerging from this trend can be broadly categorized into two\ndistinct paradigms: (1) AI-native integrated development environments (IDEs) such as Cursor [ 9]\nand Trae [ 21] that embed AI capabilities directly into the editor interface, and (2) terminal-based\nor extension-based agents including Claude Code [ 10], Gemini CLI [ 22], Github Copilot [ 23], and\nCline [ 24] that operate through command-line interfaces or editor extensions. These coding agents\nleverage a holistic understanding of the codebase to perform complex tasks such as multi-file refac-\ntoring and autonomous edits. They support flexible, composable workflows and integrate seamlessly\ninto diverse development pipelines. Commercial deployments indicate significant improvements in\nboth function implementation and overall programming productivity. Despite their effectiveness,\nthese agents suffer from context window limitations that impair their ability to process lengthy\ntechnical documents such as academic papers, and struggle to maintain coherence and correctness\nwhen synthesizing repository-level codebases.\n5.2 Scientific Coding Agents\nIn contrast to general-purpose coding agents, this class of agents targets more complex code generation\nscenarios, including the implementation and reproduction of entire codebases from high-level ideas\nand academic papers. For example, Paper2Code [ 8] addresses the research reproducibility crisis by\ntransforming machine learning papers into executable repositories. Its code generation framework\nfollows a structured three-stage process that includes system architecture design, implementation\ndetail extraction, and modular code generation. CodeScientist [ 25] generates experimental code\nfrom literature, employing an iterative generate-execute-reflect cycle to write, run, and debug Python\nexperiments. In addition, AlphaEvolve [ 26] utilize code generation for algorithmic discovery, using\nan LLM as an evolutionary mutator to propose variations to entire codebases, which are then\nrigorously evaluated. Besides, the automation code in AI Scientist [ 27] and AI-Researcher [ 6]\nenables agents to iteratively plan and execute experiments, handle errors, and refine future runs\nbased on results. AI Scientist focuses on experimental automation, maintaining execution history\nand generating plots and notes to support scientific write-ups. AI-Researcher extends this with a\nmulti-stage refinement framework, where a code agent implements modular solutions and an advisor\nagent provides structured feedback for iterative validation, revision, and scaling. These agents\nhave advanced the pace of scientific research, yet achieving higher generation efficiency without\ncompromising code quality remains an open challenge.\n14\n\n## Page 15\n\n6 Discussion: Challenges and Future Directions\nWhile DeepCode demonstrates the efficacy of principled information-flow management in high-\nfidelity repository synthesis, the transition from episodic coding tasks to autonomous, cost-effective,\nand self-evolving engineering remains fraught with challenges. We identify three critical frontiers\nthat define the future trajectory of agentic software engineering.\n(1) Agentic Capability and Computational Efficiency.SOTA performance in agentic coding\ncurrently relies on massive, proprietary LLMs (e.g. GPT-5, Claude 4.5), which incur prohibitive\ndeployment costs and high latency. Conversely, smaller, open-weight models offer efficiency but\nlack the complex reasoning capabilities required for autonomous decision-making in open-ended\nengineering tasks. Bridging this gap presents a dichotomy of challenges.(i) Fine-tuning limits:\nEnhancing small models via supervised fine-tuning (SFT) is constrained by a data bottleneck—while\nraw code is abundant, high-quality agentic trajectories are scarce and expensive to curate.(ii)\nKnowledge injection limits:Merely augmenting small models with external knowledge is often\ninsufficient; retrieved contexts may lack direct relevance to the specific coding task, and small models\nstruggle to integrate complex inputs without suffering from attention dilution.\nWe envision a shift toward hybrid agentic architectures that synergize models of varying scales, em-\nploying large models for high-level reasoning and efficient small models for routine implementation.\nBesides, distilling knowledge from large models helps reduce the data bottleneck.\n(2) From Episodic to Evolving Agents.Current coding agents typically operate in an episodic\nmanner: they reset after each project, failing to carry over experience or tacit knowledge to subsequent\ntasks. Enabling agents to self-evolve and accumulate expertise mirrors human professional growth\nbut faces significant hurdles.(i) Reinforcement Learning constraints:While RL-based optimization\ntheoretically allows agents to learn from feedback, it requires well-defined reward functions, which\nare difficult to formulate for complex, multi-objective software engineering tasks. Moreover, this",
      "content_type": "algorithm",
      "keywords": [
        "development",
        "alphaevolve",
        "modular",
        "open",
        "workflows",
        "frontiers",
        "failing",
        "paradigms",
        "synthesis",
        "designer",
        "computer",
        "deepcode",
        "notes",
        "toolgen",
        "ended",
        "understanding",
        "decompose",
        "specific",
        "contexts",
        "mutator",
        "presents",
        "experiments",
        "execute",
        "diverse",
        "interface"
      ],
      "char_start": 58061,
      "char_end": 64820,
      "char_count": 6758,
      "relevance_scores": {
        "concept_analysis": 0.59,
        "algorithm_extraction": 1.0,
        "code_planning": 1.0
      },
      "section_path": "Related Work"
    },
    {
      "id": "ea757976",
      "title": "approach",
      "content": "approach is inapplicable to closed-source LLMs where parameter updates are impossible.(ii)\nMemory scalability issues:The alternative approach—stacking historical experiences into a long-\nterm memory—introduces severe noise. Simply accumulating raw interaction logs leads to context\nbloat, where retrieving relevant past experiences becomes a “needle in a haystack” problem.\nBeyond relying on extensive manual annotation and training, a scalable solution involves automating\nthe abstraction of past experiences. Future agents can implement post-task reflection to condense\nexecution traces into reusable skills or heuristics. Storing these refined insights allows agents to\nretrieve corresponding high-level guidance, enabling self-evolution while avoiding context explosion.\n(3) Dynamic Planning and Adaptability.Most existing frameworks utilize a linear Plan-then-Code\nworkflow, assuming that all constraints are knowable a priori. In real-world engineering, specifications\noften evolve, and critical implementation constraints are frequently discovered only during the coding\nprocess. Separation between planning and execution leads to fragility: if the initial blueprint is flawed,\nthe coding agent is often constrained by a stale plan, leading to suboptimal workarounds or failure.\nFuture researches advance toward dynamic, bidirectional planning frameworks. Agents are able\nto adapt their initial blueprints when encountering unforeseen constraints during implementation.\nEstablishing a feedback mechanism where execution insights directly inform and update the high-level\nplan is crucial for handling the complex realities of large-scale software development.\n7 Conclusion\nIn this work, we presented DeepCode, an autonomous framework that advances the frontier of agentic\ncode engineering by reimagining document-to-repository synthesis as a challenge ofinformation-flow\nmanagement. Addressing the fundamental conflict between information overload and finite context\nbottlenecks, we demonstrated that treating synthesis as a channel optimization problem—solved\nthrough the orchestration of blueprint distillation, stateful memory, conditional knowledge injection,\nand closed-loop verification—effectively maximizes the signal-to-noise ratio for long-horizon tasks.\nEmpirical evaluations on PaperBench confirm that DeepCode establishes a new SOTA, decisively\noutperforming leading commercial agents and surpassing PhD-level human experts in reproduction\naccuracy. These findings validate that hierarchical information orchestration, rather than indiscrimi-\nnate context scaling, provides the decisive path toward robust autonomous systems, laying a critical\nfoundation for the future of automated scientific discovery and rigorous research reproduction.\n15\n\n## Page 16\n\nReferences\n[1]Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. A survey on large\nlanguage models for code generation.arXiv preprint arXiv:2406.00515, 2024.\n[2]Yuyao Ge, Lingrui Mei, Zenghao Duan, Tianhao Li, Yujia Zheng, Yiwei Wang, Lexin Wang,\nJiayu Yao, Tianyu Liu, Yujun Cai, Baolong Bi, Fangda Guo, Jiafeng Guo, Shenghua Liu,\nand Xueqi Cheng. A survey of vibe coding with large language models, 2025. URL https:\n//arxiv.org/abs/2510.12399.\n[3]Sida Peng, Eirini Kalliamvakou, Peter Cihon, and Mert Demirer. The impact of ai on developer\nproductivity: Evidence from github copilot.arXiv preprint arXiv:2302.06590, 2023.\n[4]Yihong Dong, Xue Jiang, Jiaru Qian, Tian Wang, Kechi Zhang, Zhi Jin, and Ge Li. A survey on\ncode generation with llm-based agents, 2025. URL https://arxiv.org/abs/2508.00083 .\n[5]Huanting Wang, Jingzhi Gong, Huawei Zhang, Jie Xu, and Zheng Wang. Ai agentic program-\nming: A survey of techniques, challenges, and opportunities.arXiv preprint arXiv:2508.11126,\n2025.\n[6]Jiabin Tang, Lianghao Xia, Zhonghang Li, and Chao Huang. AI-Researcher: Autonomous\nScientific Innovation. InNeurIPS, 2025.\n[7]Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin,\nRachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia\nGlaese, and Tejal Patwardhan. Paperbench: Evaluating ai’s ability to replicate ai research, 2025.\nURLhttps://arxiv.org/abs/2504.01848.\n[8]Minju Seo, Jinheon Baek, Seongyun Lee, and Sung Ju Hwang. Paper2code: Automating code\ngeneration from scientific papers in machine learning, 2025. URL https://arxiv.org/abs/\n2504.17192.\n[9] Anysphere. Cursor: The best way to code with ai.https://cursor.com, 2025.\n[10] Anthropic. Claude code: Agentic coding tool for your terminal. https://docs.claude.com/\nen/docs/claude-code/overview, 2025.\n[11] OpenAI. Codex cli: Pair with codex in your terminal. https://developers.openai.com/\ncodex/cli, 2025.\n[12] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize\nChen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Chatdev:\nCommunicative agents for software development, 2024. URL https://arxiv.org/abs/\n2307.07924.\n[13] Sirui Hong, Mingchen Zhuge, Jiaqi Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin\nWang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng\nXiao, Chenglin Wu, and Jürgen Schmidhuber. Metagpt: Meta programming for a multi-agent\ncollaborative framework, 2024. URLhttps://arxiv.org/abs/2308.00352.\n[14] Zeeshan Rasheed, Malik Abdul Sami, Kai-Kristian Kemell, Muhammad Waseem, Mika Saari,\nKari Systä, and Pekka Abrahamsson. Codepori: Large-scale system for autonomous software\ndevelopment using multi-agent technology, 2024. URL https://arxiv.org/abs/2402.\n01411.\n[15] Daoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Yafen Yao, Wei Li, Xiaolin Chen,\nYongshun Gong, Bei Guan, et al. Codes: Natural language to code repository via multi-layer\nsketch.ACM Transactions on Software Engineering and Methodology, 2024.\n[16] Dong Huang, Jie M. Zhang, Michael Luck, Qingwen Bu, Yuhao Qing, and Heming Cui.\nAgentcoder: Multi-agent-based code generation with iterative testing and optimisation, 2024.\nURLhttps://arxiv.org/abs/2312.13010.\n[17] Md. Ashraful Islam, Mohammed Eunus Ali, and Md Rizwan Parvez. MapCoder: Multi-agent\ncode generation for competitive problem solving. InACL, pages 4912–4944, 2024.\n16\n\n## Page 17\n\n[18] Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. CodeAgent: Enhancing code generation\nwith tool-integrated agent systems for real-world repo-level coding challenges. InACL, pages\n13643–13658, 2024.\n[19] John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik\nNarasimhan, and Ofir Press. Swe-agent: agent-computer interfaces enable automated software\nengineering. InNeurIPS, pages 50528–50652, 2025.\n[20] Renxi Wang, Xudong Han, Lei Ji, Shu Wang, Timothy Baldwin, and Haonan Li. Toolgen:\nUnified tool retrieval and calling via generation. InThe Thirteenth International Conference on\nLearning Representations, 2025. URL https://openreview.net/forum?id=XLMAMmowdY .\n[21] ByteDance. Trae: The real ai engineer.https://www.trae.ai, 2025.\n[22] Google. Gemini cli: An open-source ai agent that brings the power of gemini directly into your\nterminal.https://github.com/google-gemini/gemini-cli, 2025.\n[23] GitHub and OpenAI. Github copilot: Your ai pair programmer. https://github.com/\nfeatures/copilot, 2025.\n[24] Saoud Rizwan and others. Cline: Autonomous coding agent for vs code. https://github.\ncom/cline/cline, 2024.\n[25] Peter Jansen, Oyvind Tafjord, Marissa Radensky, Pao Siangliulue, Tom Hope, Bhavana Dalvi\nMishra, Bodhisattwa Prasad Majumder, Daniel S. Weld, and Peter Clark. Codescientist: End-\nto-end semi-automated scientific discovery with code-based experimentation, 2025. URL\nhttps://arxiv.org/abs/2503.22708.\n[26] Alexander Novikov, Ngân V ˜u, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, et al.\nAlphaevolve: A coding agent for scientific and",
      "content_type": "methodology",
      "keywords": [
        "retrieving",
        "development",
        "decisive",
        "open",
        "alphaevolve",
        "encountering",
        "mohammed",
        "forum",
        "synthesis",
        "ailun",
        "mishra",
        "yongshun",
        "waseem",
        "jiaqi",
        "deepcode",
        "frequently",
        "real",
        "zijuan",
        "computer",
        "xiawu",
        "xueqi",
        "toolgen",
        "updates",
        "transactions",
        "inneurips"
      ],
      "char_start": 64820,
      "char_end": 72653,
      "char_count": 7832,
      "relevance_scores": {
        "concept_analysis": 0.7,
        "algorithm_extraction": 0.7,
        "code_planning": 0.7
      },
      "section_path": "approach"
    },
    {
      "id": "753f51e6",
      "title": "algorithm",
      "content": "algorithmic discovery, 2025. URL https:\n//arxiv.org/abs/2506.13131.\n[27] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The\nai scientist: Towards fully automated open-ended scientific discovery, 2024. URL https:\n//arxiv.org/abs/2408.06292.\n17\n\n## Page 18\n\nA Appendix\nThis appendix supplements the main text by providing four categories of supplementary materials.\nFirst, theComplete Resultssubsection reports an extensive quantitative",
      "content_type": "methodology",
      "keywords": [
        "open",
        "robert",
        "supplementary",
        "first",
        "main",
        "jeff",
        "fully",
        "text",
        "materials",
        "scientist",
        "ended",
        "arxiv",
        "extensive",
        "quantitative",
        "appendix",
        "jakob",
        "reports",
        "cong",
        "clune",
        "david",
        "providing",
        "page",
        "tjarko",
        "resultssubsection",
        "discovery"
      ],
      "char_start": 72653,
      "char_end": 73123,
      "char_count": 469,
      "relevance_scores": {
        "concept_analysis": 0.5,
        "algorithm_extraction": 0.6,
        "code_planning": 0.5
      },
      "section_path": "algorithm"
    },
    {
      "id": "1d3e773f",
      "title": "evaluation",
      "content": "evaluation of DeepCode,\nincluding comparative analysis against multiple benchmark models and reproducibility analysis across\ndifferent papers and operational scenarios. Second, theDeepCode Application Casessubsection\nshowcases representative visualizations demonstrating DeepCode’s end-to-end capabilities, covering\nbackend systems, web user interfaces, and the Paper2Code research reproduction workflow. Third, the\nDeepCode Sub-Agent Detailssubsection elucidates the internal multi-agent architecture, clarifying\nthe roles, responsibilities, and coordination patterns for implementing specific specialized sub-agents.\nFinally, theMCP Toolkit in DeepCodesubsection documents the Model Context Protocol (MCP)\ntools integrated into the system, defining the external interfaces through which DeepCode interacts\nwith code repositories, documentation, and execution environments.\nA.1 Full",
      "content_type": "experiment",
      "keywords": [
        "systems",
        "code",
        "internal",
        "against",
        "multiple",
        "scenarios",
        "environments",
        "deepcode",
        "benchmark",
        "specific",
        "into",
        "showcases",
        "roles",
        "representative",
        "backend",
        "application",
        "operational",
        "finally",
        "interacts",
        "repositories",
        "architecture",
        "patterns",
        "covering",
        "workflow",
        "coordination"
      ],
      "char_start": 73123,
      "char_end": 74007,
      "char_count": 883,
      "relevance_scores": {
        "concept_analysis": 0.5,
        "algorithm_extraction": 0.5,
        "code_planning": 0.6
      },
      "section_path": "evaluation"
    },
    {
      "id": "9c3ec94f",
      "title": "Result",
      "content": "Results\nThis appendix reports quantitative results that complement the main text and provide a more systematic\nevaluation of DeepCode’s overall capability and stability on research code reproduction tasks. Table 2\nfirst compares, under a unified evaluation protocol, a range of general-purpose code execution agents\n(including both BasicAgent and IterativeAgent configurations), existing specialized reproduction\nsystems such as PaperCoder, and human experts on the same benchmark. DeepCode achieves\nan average reproduction score of 73.5±2.8 on the full benchmark, substantially outperforming\nPaperCoder ( 51.1±1.4 ) as well as all configurations derived from commercial models. On the\n3-paper subset, DeepCode attains an average score of 75.9±4.5 , exceeding the human “Best@3”\nscore of 72.4, indicating that, on representative deep learning papers, the system delivers reproduction\nquality comparable to or better than that of strong human practitioners.\nTable 1 further selects a 5-paper subset (fre, rice, bam, pinn, mech-u) for a head-to-head comparison\nagainst several widely used commercial code assistants (Codex, Claude Code, Cursor, etc.). Across all\npapers, DeepCode achieves the highest reproduction score, with an average of 0.8482, corresponding\nto an absolute improvement of more than 0.26 over the strongest competing system. The advantage\nis consistent across all individual papers, suggesting that the gains arise from architectural and\nprocedural design choices rather than from favorable alignment with a narrow subset of tasks.\nFinally, Table 3 provides per-paper details for the Claude 4.5 Sonnet–based configuration, includ-\ning three independent runs, their mean and standard error, as well as the associated average cost.\nAcross a diverse set of targets—such as FRE, PINN, MECHANISTIC-UNDERSTANDING, and\nSEQUENTIAL-NEURAL-SCORE-ESTIMATION—DeepCode’s reproduction scores typically lie\nin the 0.7–0.9 range with relatively small standard errors, while the distribution of average cost\nacross papers remains tight. This indicates strong cross-task generalization, stable behavior across\nrepeated runs, and reasonable resource usage. Taken together, these appendix results reinforce the\nmain",
      "content_type": "experiment",
      "keywords": [
        "systems",
        "attains",
        "substantially",
        "code",
        "typically",
        "indicating",
        "stability",
        "existing",
        "task",
        "configurations",
        "behavior",
        "against",
        "sonnet",
        "pinn",
        "achieves",
        "commercial",
        "indicates",
        "better",
        "over",
        "deepcode",
        "remains",
        "widely",
        "benchmark",
        "tasks",
        "together"
      ],
      "char_start": 74007,
      "char_end": 76220,
      "char_count": 2212,
      "relevance_scores": {
        "concept_analysis": 0.5,
        "algorithm_extraction": 0.5,
        "code_planning": 0.7
      },
      "section_path": "Result"
    },
    {
      "id": "4912789a",
      "title": "conclusion",
      "content": "conclusions of the paper: on realistic research code reproduction benchmarks, DeepCode not\nonly achieves significantly higher average performance than existing automated reproduction and\ncode assistance systems, but also demonstrates robust and consistent advantages in fine-grained,\nmulti-paper, multi-run analyses.\nA.2 Use Cases for DeepCode\nThis appendix provides a series of visual artifacts generated by DeepCode, offering concrete evidence\nof its capabilities across different software development and research domains. These examples\nare intended to supplement the main paper by illustrating the practical utility and versatility of our\nsystem.\nThe initial set of examples, depicted in Figure 7, focuses on DeepCode’s proficiency in generating\nsophisticated backend systems. The figures showcase automatically constructed administrative\ndashboards, which likely include functionalities for data monitoring, user management, and content\nmoderation. Such pages are critical for the operational management of modern web applications but\nare often tedious and repetitive to build. DeepCode’s ability to scaffold these complex, data-driven\ninterfaces from high-level specifications demonstrates its potential to significantly reduce boilerplate\nengineering and accelerate the deployment of robust server-side infrastructure.\n18\n\n## Page 19\n\nTable 2: Average reproduction scores: DeepCode vs. LLMs and human experts\nModel Average Replication Scores\nGEMINI-2.0-FLASH (BasicAgent)5.0±0.0\n4o (BasicAgent)7.7±0.0\no3-mini (BasicAgent)5.1±0.8\no1 (BasicAgent)19.5±1.2\nR1 (BasicAgent)9.8±0.0\nCLAUDE-3-5-SONNET (BasicAgent)35.4±0.8\no3-mini (IterativeAgent)16.4±1.4\no1 (IterativeAgent)43.3±1.1\nCLAUDE-3-5-SONNET (IterativeAgent)27.5±1.6\no1 [36 hours] (IterativeAgent)42.4±1.0\nPaperCoder51.1±1.4\nDeepCode 73.6±5.3\nHuman[3 paper subset, Best@3] 72.4\nDeepCode[3 paper subset, Average]76.7±3.9\nTable 3: DeepCode with Claude 4.5 Sonnet results.\nPaper Run 1 Run 2 Run 3 Mean Std. Error Avg. Cost\nFRE 0.844 0.823 0.803 0.814 0.020 9.14\nRICE 0.738 0.609 0.761 0.702 0.082 8.22\nBAM 0.853 0.673 0.719 0.748 0.094 8.45\nWILL-MODEL-FORGET 0.776 0.793 0.857 0.808 0.042 9.20\nPINN 0.947 0.800 0.983 0.910 0.097 7.84\nALL-IN-ONE 0.769 0.747 0.759 0.759 0.011 9.43\nADAPTIVE-PRUNING 0.547 0.570 0.516 0.544 0.027 9.13\nLBCS 0.689 0.732 0.820 0.747 0.066 10.01\nMECHANISTIC-UNDERSTANDING 0.889 0.944 0.941 0.925 0.031 10.20\nTEST-TIME-MODEL-ADAPTATION 0.717 0.578 0.652 0.649 0.069 7.90\nSAMPLE-SPECIFIC-MASKS 0.690 0.740 0.583 0.671 0.080 8.30\nBRIDGING-DATA-GAPS 0.552 0.566 0.626 0.581 0.039 7.98\nSTAY-ON-TOPIC-WITH-CLASSIFIER-FREE-GUIDANCE 0.734 0.800 0.626 0.705 0.088 9.12\nSTOCHASTIC-INTERPOLANTS 0.851 0.792 0.801 0.815 0.031 8.89\nLCA-ON-THE-LINE 0.665 0.844 0.739 0.749 0.090 7.73\nSEQUENTIAL-NEURAL-SCORE-ESTIMATION 0.930 0.862 0.817 0.870 0.057 10.01\nSAPG 0.702 0.755 0.757 0.738 0.031 9.19\nFTRL 0.558 0.606 0.631 0.598 0.037 7.06\nROBUST-CLIP 0.772 0.742 0.685 0.733 0.044 7.83\nBBOX 0.620 0.681 0.631 0.644 0.033 11.90\nBuilding upon the backend logic, a system’s utility is often defined by its user-facing presentation.\nFigure 8 illustrates DeepCode’s capacity for generating intuitive and functional Web UIs. The\ngenerated interfaces, featuring elements such as data visualization charts and interactive forms,\ntranslate abstract user requirements into tangible, interactive components. This capability not only\ncomplements the backend generation by providing a corresponding frontend, but also empowers\ndevelopers and designers to rapidly prototype and iterate on user experiences, thereby shortening the\npath from concept to a functional product.\nPerhaps DeepCode’s most ambitious application, however, lies in its potential to bridge the chasm\nbetween academic research and practical implementation. The Paper2Code functionality, illustrated\nin Figure 9, exemplifies this capability. The figure is twofold: on the left, it presents the high-level\ncode structure that DeepCode inferred from a research paper, discerning the architectural blueprint\nof the proposed algorithm, including its modular components and file organization. On the right, it\nprovides a concrete code sample, instantiating a specific function or class with precise logic. This\npowerful feature moves beyond conventional code generation by interpreting unstructured scientific\n19\n\n## Page 20\n\nFigure 7: DeepCode-generated backend system pages.\nFigure 8: DeepCode-generated Web UI.\nlanguage to produce structured, executable artifacts, thereby holding immense promise for enhancing\nresearch reproducibility and accelerating the adoption of novel scientific discoveries.\nA.3 Sub-Agents Details of DeepCode\nDeepCode decomposes the software engineering pipeline into a set of specialized agents with narrow,\nwell-specified responsibilities and standardized communication interfaces, rather than relying on a\nsingle monolithic generative model. The individual agents and their responsibilities are summarized\nin Table 4. This modular design allows different stages of the lifecycle—ranging from requirement un-\nderstanding to architectural planning and code synthesis—to be implemented as transformations over\nshared intermediate representations, while preserving global architectural and semantic consistency.\nDuring the planning stage, DeepCode relies on explicit coordination between conceptual and\nalgorithmic analysis agents to derive a coherent development blueprint from high-level spec-\nifications.The Central Orchestrating Agent first routes each input through the Document Parsing\nand/or Intent Understanding agents to obtain a structured specification, which then serves as the\ninput to the Code Planning agent. Within this planning module, two internal analysis pipelines\noperate in parallel over the same intermediate representation. The conceptual analysis sub-agent is\nresponsible for system-level decomposition: it identifies major subsystems, their responsibilities,\nand inter-module interfaces, and it constructs an architecture-level call topology. The algorithmic\nanalysis sub-agent is responsible for computational aspects: it abstracts key algorithmic ideas, selects\ncandidate data structures, reasons about time and space complexity constraints, and enumerates\nfeasible implementation patterns. The partial plans produced by these two sub-agents are reconciled\nby a planning aggregation component (Code Analysis agent), which resolves inconsistencies and\nmaterializes a project-level development roadmap, including module boundaries, interface signatures,\ndependency relations, implementation priorities, and testing hooks. This roadmap serves as the design\nbaseline that constrains all downstream code generation and refinement steps.\n20\n\n## Page 21\n\nFigure 9: Paper2Code Samples of DeepCode. Left: Code Structure, Right: Code Sample\nDuring the code synthesis stage, DeepCode couples retrieval-augmented",
      "content_type": "conclusion",
      "keywords": [
        "development",
        "modular",
        "resolves",
        "versatility",
        "forms",
        "synthesis",
        "potential",
        "visualization",
        "priorities",
        "identifies",
        "time",
        "achieves",
        "instantiating",
        "deepcode",
        "chasm",
        "steps",
        "understanding",
        "specific",
        "baseline",
        "forget",
        "illustrates",
        "most",
        "utility",
        "presents",
        "pipeline"
      ],
      "char_start": 76220,
      "char_end": 83093,
      "char_count": 6872,
      "relevance_scores": {
        "concept_analysis": 0.6,
        "algorithm_extraction": 0.7,
        "code_planning": 0.9
      },
      "section_path": "conclusion"
    },
    {
      "id": "48c28454",
      "title": "reference",
      "content": "reference mining with\na global code memory, forming a closed-loop process that enforces repository-level consistency\nduring incremental generation.On the retrieval side, the Code Reference Mining and Code Indexing\nagents implement a Retrieval-Augmented Generation (RAG) layer: they maintain multi-granularity\nindices over a corpus of prior implementations and expose to the Code Generation agent semantically\nrelevant and structurally compatible code patterns, ranging from individual functions to reusable\ndesign idioms. In parallel, the Code Memory agent maintains a structured representation of the\ncurrent repository state, including cross-file symbol tables, dependency graphs, and project-wide\nconventions such as naming schemes, error-handling strategies, and configuration mechanisms.\nBefore emitting new code, the Code Generation agent issues queries to the Code Memory agent to\nobtain the up-to-date repository context and applicable constraints; after generation, it writes back\nthe newly introduced symbols and dependencies, triggering an update of the global repository model.\nThis query–constraint–update loop allows DeepCode to align local synthesis decisions with global\narchitectural intent, reducing interface mismatches, naming drift, and latent coupling across the\ncodebase.\nA.4 MCP Tool Stack in DeepCode\nTable 5 summarizes the Model Context Protocol (MCP) tools integrated into DeepCode. The tools are\ngrouped into three functional categories:Perception & Retrieval,Cognitive Processing, andAction &\nExecution. This organization makes the main stages of the system explicit. Perception & Retrieval\ntools give the model access to up-to-date web search results, web pages, and binary documents such\nas research papers and technical manuals, which helps mitigate the effects of the model’s knowledge\ncut-off. Cognitive Processing tools then convert large codebases and long documents into semantic\nindexes and context-window-compatible segments, so that the model can issue natural language\nqueries over existing artifacts and work with long technical materials. Action & Execution tools\nfinally operate on the local development environment by reading and writing project files, executing\nshell commands, and interacting with the version control system.\nTaken together, the tools in Table 5 form an end-to-end loop for assisted software development. The\nsystem can retrieve external and local information, reorganize it into internal structures that fit within\nthe model’s context window, and then apply code changes while observing their effects through\ncommands such as tests or package installations. The table also shows that operations with side\neffects on the environment (file I/O, command execution, and Git operations) are confined to the\nAction & Executionlayer and are described as sandboxed and path-validated. This separation between\ninformation access, semantic processing, and environment manipulation makes the extension of the\nbase language model through MCP tools transparent and easier to reason about.\n21\n\n## Page 22\n\nTable 4: Functional Specifications of Specialized Sub-Agents in the DeepCode Framework\nAgent Role Functional Description\nCentral Orchestrating\nAgentFunctions as the central control unit, responsible for task decomposi-\ntion, resource allocation, and the strategic coordination of sub-agents\nbased on the complexity of the input requirements.\nIntent Understanding\nAgentConducts semantic parsing of natural language inputs to extract\nfunctional requirements, converting ambiguous user descriptions\ninto formal technical specifications.\nDocument Parsing Agent Processes unstructured technical documents (e.g., research papers).\nIt extracts multimodal information, including text, mathematical\nformulas, and diagrams, to establish a ground truth for implementa-\ntion.\nConcept Analysis Agent Abstracts core theoretical concepts and logical flows from the parsed\nspecifications, ensuring the computational model aligns with the\ntheoretical underpinnings of the source material.\nAlgorithm Analysis Agent Evaluates and selects appropriate algorithmic strategies and data\nstructures. It focuses on optimizing computational complexity and\nfeasibility before code synthesis begins.\nCode Planning Agent Formulates the software architecture and development roadmap.\nThis agent determines the technology stack, designs modular file\nstructures, and enforces design patterns to ensure scalability.\nCode Reference Mining\nAgentRetrieves external knowledge by identifying relevant open-source\nrepositories. It analyzes dependency graphs to recommend integra-\ntion patterns and library usages.\nCode Memory Agent Manages the state and context throughout the generation lifecycle.\nIt utilizes hierarchical data structures to retain historical decisions\nand maintain semantic consistency across long-context interactions.\nCode Generation Agent Synthesizes executable source code based on the architectural plan\nand retrieved references. It implements functional interfaces and\nintegrates distinct modules into a cohesive codebase.\nAutomated Validation\nAgentExecutes a rigorous quality assurance loop. It performs static analy-\nsis, generates unit tests, and iteratively debugs the codebase to verify\nfunctional correctness and adherence to specifications.\n22\n\n## Page 23\n\nTable 5: Specification of Model Context Protocol (MCP) Tools Integrated into DeepCode. These\ntools extend the Large Language Model’s capabilities across perception, cognitive processing, and\nenvironment manipulation domains\nCategory MCP Server Name Functional Description & Academic Specifi-\ncation\nPerception & Retrievalbrave_search A real-time information retrieval interface lever-\naging the Brave Search API. It provides the\nagent with temporal-aware access to web in-\ndices, enabling the retrieval of up-to-date doc-\numentation and resolving knowledge cut-off\nlimitations.\nbocha_mcp A specialized search module delivering struc-\ntured \"modal cards\" and semantic summaries.\nIt serves as a secondary knowledge source, opti-\nmizing token efficiency by returning structured\nentities rather than raw HTML.\nfetch A web content ingestion engine that retrieves\nURL endpoints and normalizes heterogeneous\nHTML structures into clean Markdown. It acts\nas the agent’s primary reading interface for ex-\nternal documentation.\npdf_downloader Binary resource acquisition tool designed for\nacademic papers and technical manuals. It han-\ndles HTTP streams to ingest non-textual doc-\nument formats (PDF/DOCX) for downstream\nprocessing.\nCognitive Processingcode_reference_indexer A Retrieval-Augmented Generation (RAG)\nmodule for local codebases. It constructs a\nvector or semantic index of the project files,\nallowing the agent to perform natural language\nqueries over the existing code structure.\ndocument_segmentation A pre-processing utility implementing semantic\nchunking algorithms. It partitions large techni-\ncal documents into context-window-compliant\nsegments, facilitating the \"Paper2Code\" work-\nflow for complex algorithm implementation.\nAction & Executionfilesystem A sandboxed file I/O interface allowing con-\ntrolled read/write operations within the project\ndirectory. It enforces path validation security\npolicies to prevent unauthorized system access\nduring code generation.\ncode_implementation The core generative engine encapsulated as an\nMCP tool. It orchestrates the synthesis of func-\ntional code blocks, integrating logic planning\nwith atomic file writing operations to ensure\ncode coherence.\ncommand_executor A runtime environment interface permitting the\nexecution of shell commands (e.g., pytest ,\npip install ). It establishes a feedback loop\nby capturing stdout /stderr for iterative de-\nbugging and self-correction.\ngit_command Version control management interface. It ab-\nstracts Git plumbing commands, enabling the\nagent to manage repository state, branch for ex-\nperimental features, and maintain a clean com-\nmit history.\n23",
      "content_type": "references",
      "keywords": [
        "development",
        "open",
        "modular",
        "synthesis",
        "time",
        "indexes",
        "triggering",
        "deepcode",
        "agentconducts",
        "real",
        "understanding",
        "mining",
        "source",
        "designs",
        "utility",
        "reducing",
        "appropriate",
        "name",
        "package",
        "acts",
        "unauthorized",
        "evaluates",
        "delivering",
        "interface",
        "coherence"
      ],
      "char_start": 83093,
      "char_end": 91043,
      "char_count": 7948,
      "relevance_scores": {
        "concept_analysis": 0.7,
        "algorithm_extraction": 0.7,
        "code_planning": 0.9
      },
      "section_path": "reference"
    }
  ],
  "created_at": "2025-12-16T23:47:12.968661"
}